#+TITLE: hft project
#+OPTIONS: toc:2 num:nil ^:nil
 
* hft thread
** TODO hft boot
SCHEDULED: <2012-12-18 Tue>
:PROPERTIES:
:ID:       FDF55DAB-2476-4741-B4AF-065013EF50DE
:END:
- [ ] find R project details
- [ ] review internal material
- [X] audit linkedin threads
- [X] audit github
- [X] get tradelink up
  - [X] VM
  - [X] VS2010
- [ ] review tick data
  - [ ] revolution packages

** TODO c++ compilation
:PROPERTIES:
:ID:       E42F6BAC-B044-47F6-BC69-950F5C0389E3
:END:
- [ ] hello world
- [ ] xcode up
- [ ] reverse g++ -> llvm-g++-4.2 deleted (?)
  - [ ] what is llvm?
- [ ] get lockfree++ compiling
- [ ] http://libcppa.blogspot.com.au/2012/07/libcppa-on-mountain-lion.html







** TODO c++ reference
:PROPERTIES:
:ID:       5A96BE90-2710-4B38-8D3D-87B3E0C18510
:END:

http://www.quantinvest.com.br/knowledge/tic02/html/TicV2.html

** TODO code
:PROPERTIES:
:ID:       6C7754E7-1724-4F75-99E2-A43EA7FB95AA
:END:

http://triceps.sourceforge.net


** TODO file this
:PROPERTIES:
:ID:       6181731D-5806-4271-A329-C7367CF639B0
:END:
http://code.google.com/p/cep-trader/

** TODO reading
:PROPERTIES:
:ID:       C9E3E178-0306-43F1-A558-7D3A9B15E996
:END:
http://kenai.com/downloads/javafx-sam/EventProcessinginAction.pdf


** TODO http://www.rinfinance.com/agenda/2010/MariaBelianina.pdf
:PROPERTIES:
:ID:       DAECC838-588E-4027-84C4-0E211D29AEC2
:END:


** TODO flow toxicity                                              :reading:
:PROPERTIES:
:ID:       2443D93A-D514-4E32-AE87-9AA67AE41BD8
:END:

[[file:~/git/quaff/Flow%20Toxicity%20and%20Liquidity%20in%20a%20High%20Frequency%20World.pdf][file:~/git/quaff/Flow Toxicity and Liquidity in a High Frequency World.pdf]]

** TODO #+begin_src octave
:PROPERTIES:
:ID:       1B99AAB1-94B5-4AA3-BCB1-CB0D4848C7B3
:END:

#+end_src
rtino reading

** TODO http://spin.atomicobject.com/2012/11/15/message-oriented-programming/
:PROPERTIES:
:ID:       56138430-C9A7-4A76-8E5D-2743F5A592B6
:END:

** TODO http://www.cs.berkeley.edu/~vazirani/algorithms/all.pdf
:PROPERTIES:
:ID:       D2C92C26-9D2B-4AD2-913F-EDB52B6F39C5
:END:

** TODO mail algo note
:PROPERTIES:
:ID:       6AD7689E-1745-4896-987C-6A1FC9763D91
:END:

I'm snowed under by work at the moment so I don't think I can learn esper script anytime soon.  I think I've forgotten sql lol.
But I have been intensively learning lisp and R recently, which is a dangerous combination to learn at the same time, so here's my pseudo-code dump of the idea:


step 1: boiler-plate 

Most common technical analysis in the world: extracting a signal using short-run and long-run moving average of price:

n.short = 100000
n.long = 1000000
signal = sum(old.data.stream[1::n.long]) / n.long - sum(old.data.stream[1::n.short]) / n.short

step 2:

Both the long and the short ma are calculated on the old.data.stream and are just different weighting schemes.

weight = matrix(1/n.long, 1, n.long) - matrix (1/n.short,1,n.short)
signal = sum(weight * old.data.stream[1::nlong])

step 3:

The weighting scheme:
- can be much more general than restricted to two simple moving averages
- can represent a large subset of technical trading rules
- artificially weights data that is one million ticks old the same as the last tick.

So,

better.weighting.scheme = prior.smooth.curve.declining.to.zero
optimise(better.weighting.scheme)
signal = sum(better.weighting.scheme * old.data.stream[1::n.long])

step 4:

convert the signal to a vector method involving old values of itself (this is a tricky bit)

signal[0] = some.function(tricky.weighting.scheme, signal[1:nlong], old.data.stream[1:n.long])

Remember that old signal values are also just weighted sums of old.data.stream, so working out the tricky.weighting.scheme is pretty much just tricky diff algebra.

It also means that some.function looks pretty similar to just a sum (but there might be some non-linearity there)

step 5:

Find that the importance of older old.data.stream values decays very quickly once you start to use past signal values.  So quickly, in fact, that old.data.stream[3] (say) has a fairly low weight even!  The importance of signal[1:n.long] decays less quickly.

So the signal calculation becomes:

signal[0] = sum(tricky.weighting.scheme, signal[1:n.not.so.long], old.data.stream[1:3])

step 6:

Iterate steps 3,4 & 5 on the *signal* (which is itself now an algerbaic iteration of a weighting scheme), each time coming up with a new weighting scheme that reduces the dependence on old data.

The new weighting scheme can also be thought of as a new signal.

tricky.weights[0]=tricky.weighting.scheme
signals[0] = signal
for (x1=1:20)
   tricky.weights[x1] = optimise(minimise(n.not.so.long), signals[0:(x1-1)],tricky.weights[0:(x1-1)])
   signals[x1] = some.function(signals, tricky.weights)
end for

Now step 6 is the very tricky bit I havent fully thought through.  I doubt a c loop will work for example.  And I'm probably trying to reinvent something that already exists, like a principle component method or something.  But what I think this process converges to is this:

signals[0] = some.linear.function(signals[1], old.data.stream[0])

=> signals = funk(signals, linear.signal.functions, data.stream)

Finally,

There are a few other narratives which support this:

The end result looks very neat from a mathematical and computational point-of-view.  This is the way the world is supposed to look, with an event stream, an information state (the signals) and an algorithm that relates event to change in state.

Having done all the tricky math, it's easy to relate the original algorithm to the final result.  In the original method:
- there are 1,000,001 signals: 1,000,000 being the last one million prices and 1 being the calculated signal.
- there is 1 linear.function, the dual ma crossover.
Or you could think about it as 3 signals (long ma, short ma and the difference) in addition to the old.data.

The whole exercise could be thought of a combined compression and optimisation computation.  Some parts of the story are simply about compressing the market data so it can compactly sit on the event stream.  The algorithm needs to be transformed given it needs to operate on compressed data but it should spit out about the same answer (or there will be a speed - accuracy tradeoff).  But some parts of the story are about looking at the algorithm in the light of getting it on the stream and thinking about how to do it better.


I would like to try putting algorithms in the event stream as part of an actor framework. I need to more rigorously define what I mean by this. One way to describe it is that algorithm actors in the stream (functions/transforms/state-variables) cant use stream history to recompute themselves. They can only use other actors in the stream. 

As an example, consider a moving average. Most algorithms recalculate every time so they look at the last million or so ticks (data events) every time they update themselves. Some might be smart enough to add the latest data and drop off old values. But a true streaming algorithm doesnt have to remember old values and can use an exponential decay method to keep track of the moving average. Thus the moving average algorithm becomes a state variable in the stream. And it also goes from being a summation of a million values to being a few bit shifts ;) 

Does any of this make sense? 

I agree with you about algo-traders and I'm going to spend some time looking closely at the project to learn how to put something like this together. 

CASE tool like http://www.andromda.org/ 

http://www.tradewithvelocity.com/ , or http://ironbeam.com/ . It competes with Ninja, NewpageSoft, Iqbroker.com [

http://algo-trader.googlecode.com

** TODO activequant
:PROPERTIES:
:ID:       86680DB6-A0F8-4A7F-994E-84C3ADE5A538
:END:
active quant:

a server that provides auxiliary support, aggregating ticks, cep on
data, book keeping, risk management etc.



* clipboard


** raw linked in guff

*** algo
My starting point for algorithm taxonomy is a little different. To bounce some ideas off of your list: 

+ a major classification IMO is the extent that algorithms use factors exogenous to the price series (eg earnings news flow, company mentions on twitter) versus endogenous price factors (eg moving average and volatility calculations). 
+ is there a major class in between pure exogenous and pure endogenous, which is often called statistical arbitrage? Another way of saying this is that, to varying degrees, algorithmic devices are often implicit searches for orthogonal factors that better represnt what assets are (think return = beta + alpha). 
+ can I say "I have no idea" to the question of which market inefficiency, but still accurately classify algorithms? Why an algorithm 'works' or not I see as an empirical question that you can then apply theory to. 
+ are the securities involved, the specialist domain, leverage involved and the time scale just surface-level details? Is there such a thing as an algorithm that could only work on a particular security? Or just that using the algorithm in a certain way is questionable in theory or practice. 

And I think you hit on one of my personal priors: that backtesting doesn't work (because that's what everyone else does). 

To summarise, I think that many algorithmic distinctions are just semantics and parameter estimation. Isn't mean reversion just momentum with a negative parameter? A garch model is just a weighted moving average of volatility. Is the algorithmic concept of carry in currency the same as carry in fixed interest? Is the concept of dividend yield the carry concept for stocks? 

And yes, a main use of an algorithmic classification scheme would be to make algorithm selection itself an algorithm ;)

My background is much more the long-term time scale perspective and I agree algorthims can scale up or down in the time dimension.

@ofer, the examples you used suggested more that at different time horizons traders utilise different information sets and structures, and maybe it's not so much time horizon that distinguishes and categorises algorithms as it is the data-set that the algorithm is applied to.

@Tom thanks for the link and the forum looks inetersting. Isn't trend extrapolation (predictive 1) and contrarian, breakout (predictive 2) just the same sort of algorithm but with opposite sign parameters? I dont get non-predictive. It's still predictive, just looking for higher-probability near-arbitrage payoff distributions.

I'd like to know more about entry versus exit characteristics. Is there really such a dichotomy or is this just a semantic distiction, and its just that different parameters should be used once a position is 'live'.

Obviously an entire trading system can be thought of as an 'algorithm' that turns data into a profit stream. I would like to peel that back a bit and exclude orthogonal considerations as you suggest like accounting, risk and objectives management. Can I suggest that there is a divide in the data inputs to the trading system: 
1. data that is individual to the trader and situation specific. (P&L distribution choices, risk metrics, accounting, market segment tastes, jargon-centric knowledge)

1. data that is external and independent of the specific trading situation.

I'd like to restrict the classification to algorithms that apply more to the universe of securities than algorithms that apply to a traders situation.

But maybe this separation is impossible or would lead to a sub-optimal approach.

*** sornette

http://arxiv.org/find/all/1/all:+sornette/0/1/0/all/0/1



the site for all his articles
http://arxiv.org/find/all/1/all:+sornette/0/1/0/all/0/1
<http://arxiv.org/find/all/1/all:+sornette/0/1/0/all/0/1>

some specific articles

http://arxiv.org/pdf/cond-mat/0301543.pdf
<http://arxiv.org/pdf/cond-mat/0301543.pdf>
http://arxiv.org/pdf/1108.0077.pdf <http://arxiv.org/pdf/1108.0077.pdf>
http://arxiv.org/ftp/arxiv/papers/1012/1012.4118.pdf
<http://arxiv.org/ftp/arxiv/papers/1012/1012.4118.pdf>

http://arxiv.org/pdf/1011.2882.pdf <http://arxiv.org/pdf/1011.2882.pdf>
http://arxiv.org/pdf/1007.2420.pdf <http://arxiv.org/pdf/1007.2420.pdf>
http://arxiv.org/pdf/0909.1007.pdf <http://arxiv.org/pdf/0909.1007.pdf>

http://arxiv.org/ftp/arxiv/papers/0812/0812.2449.pdf
<http://arxiv.org/ftp/arxiv/papers/0812/0812.2449.pdf>

http://www.er.ethz.ch/people/sornette
<http://www.er.ethz.ch/people/sornette>

*** open source projects of interest

https://github.com/penberg/libtrading

https://github.com/dakka/fix8

http://fix8.org

https://github.com/odeheurles/Disruptor-net
http://disruptor.googlecode.com/files/Disruptor-1.0.pdf

http://www.aurorasolutions.org/over-6-million-transactions-per-second-in-a-real-time-system-an-out-of-the-box-approach/


http://programmers.stackexchange.com/questions/121592/what-to-look-for-in-selecting-a-language-for-algorithmic-high-frequency-trading

http://code.google.com/p/tradelink/

http://tradexoft.wordpress.com/

1. algorithms: I'm trying to get some discussion and collaboration 
going with respect to an algorithm classification scheme. I would like to code up some algorithmic approaches that: 
- are security detail agnostic (ie can be applied to any tradable asset) 
- are divided into three categories: functions of historical price 
(technicals), functions of the historical prices of other 
securities (stat arb) and functions of factors exogenous to price 
(fundamentals). 

2. prototype: I like your idea of reusable components but I think the 
group would need a working prototype. I'm new to high-frequency 
trading and I personally need to build and run a complete system to 
develop intuition. I suspect there are many others in the same boat. 

I think a github project would be ideal for collaboration. It 
could simply be a meta-project with the sub-projects existing 
separately.

http://esper.codehaus.org

The financial industry contains many people that have no clue about software and which want to reinvent the wheel, because of various reasons (we could go on for quite a while). Mind that I don't think about you guys ... but there are quite a bunch of people that say "we do everything in-house" without seeing that only 5-15% of their code and software is really unique, the rest is boilerplate code, like what you say about bootstrap.

What makes sense though: 
Define a communication protocol between different components.

I use Google Protocol Buffer messages within the architecture. Check out: http://activequant.org/svn/aq2o/trunk/base/src/main/proto/messages.proto

It's a subset of many possible messages. It are the messages that I need in my system.

Things like FIX won't fly. FAST is already better.

By Protocol Buffers you mean http://code.google.com/p/protobuf-net/ 
+1 for speed

By FAST do you mean this? https://code.google.com/p/quickfast/ 
+1 for speed

By keeping the protocols and communication interfaces between these parts language agnostic, a chap like me can do more or less whatever he likes in his own module or sandbox. 
you might also go for a monolithic block to cut out the messaging layer, but then you'll run into other trouble ... 

i think you pals should use some framework and replace parts of it with your own stuff and then take it from there. for example, write a message consumer for the protocol buffers that I posted, then you can already connect to an ActiveQuant master server and have some live data flowing back and forth, etc. If you are eager you can replace the server later with your own structure ... but then you might at one point want to record all order events for auditing purposes ... which is already implemented in the ActiveQuant Master Server ... 

As said, I think you should use some open source platform and extend it. 

Looking at this tradexoft story, I can imagine you want to create a connector to AQMS, receive market data from it and route order events ... When done with that, you could replace it in case of need with a connectivity to NASDAQ, whatever. (Let's just skip the discussion whether it makes economical sense at all and whether the skill set required and the pocket depth required to actually trade really HFT on Nasdaq will bring you a single user ... colocations, etc. come into mind)

http://stackoverflow.com/questions/731233/activemq-or-rabbitmq-or-zeromq-or

http://wiki.msgpack.org/pages/viewpage.action?pageId=1081387

https://github.com/Neverlord/libcppa

Uau!!! it's new for me, an incredible framework... I took a look a the code, and it is the first open-source code that I see that has succeed in using a CASE tool like http://www.andromda.org/ to generate code automatically. Anyway, the dependency injection pattern http://en.wikipedia.org/wiki/Dependency_injection I learnt from the last article you posted. This is a nice reference for the design patterns http://dofactory.com/Patterns/Patterns.aspx . 

this framework as a software to be offer to Brazilian funds and brokers seems to be an optimal solution, but as an ultra-low-latency system for HFT, I would say that does not fit... the name says: algo-trader.. to run on Interactive Brokers... it is not http://www.tradewithvelocity.com/ , or http://ironbeam.com/ . It competes with Ninja, NewpageSoft, Iqbroker.com [take a look, Yuval is my friend], etc... 

For HFT I would think about C++, ZeroMQ, MongoDB under an Actor Framework. 
http://en.wikipedia.org/wiki/Actor_model 

For sure the algo-trader framework should be used as a prototype of the ultra-low-latency solution. 





On 11/15/12 4:40 PM, Tony Day wrote: 
-------------------- 
Hi Henry, 

Have you ever looked at algo-traders? http://algo-trader.googlecode.com 





*** sornette notes

ln[E[p(t)]] = a + b (tc -t)^m  b<0 0<m<1

log periodic power law (LPPL) model (Sornette,
2003a,b; Zhou, 2007).

ln[p(t)] = A + Bx^m + Cx^m cos(w ln x + )
where x = tc−t measures the time to the critical time tc. For 0 < m < 1 and B < 0 (or m ≤ 0 and B > 0

practical paper

http://arxiv.org/pdf/0909.1007.pdf  p5 gives an optimisation approach

Taboo search + levenberg-marquardt

*** bunny hackathon
:LOGBOOK:
CLOCK: [2012-11-03 Sat 12:37]--[2012-11-05 Mon 14:11] => 49:34
:END:

- rabbitmq
- erlang
- interactive
- R

**** rabbitmq
- brew info

#+begin_src sh
mkdir -p ~/Library/LaunchAgents
cp /usr/local/Cellar/rabbitmq/2.8.7/homebrew.mxcl.rabbitmq.plist ~/Library/LaunchAgents/
launchctl load -w ~/Library/LaunchAgents/homebrew.mxcl.rabbitmq.plist
#+end_src



**** R

IBrokers package
http://cran.r-project.org/web/packages/IBrokers/vignettes/IBrokers.pdf



#+begin_src R
  require("IBrokers")
#+end_src

#+results:
: TRUE


**** platforms

Open Source / Free
ActiveQuant http://www.activestocks.de/
EclipseTrader http://www.eclipsetrader.org/
AIOTrade http://blogtrader.net/page/dcaoyuan/category/AIOTrade
Merchant of Venice http://mov.sourceforge.net/
Open Java Trading System http://ojts.sourceforge.net/
TrueTrade http://code.google.com/p/truetrade/
Artificial Stock Market http://artstkmkt.sourceforge.net/
iTrade http://itrade.sourceforge.net/
Matlab Automated Trading Toolbox http://sourceforge.net/projects/mlmechtrade
NexTick http://nextick.sourceforge.net/
Robotrader http://sourceforge.net/projects/robotrader/
TickZOOM http://www.tickzoom.org
Marketcetera http://www.marketcetera.com/


http://www.elitetrader.com/vb/printthread.php?threadid=148249

Using IBrokers from R is going to be the easiest route. A quick
example of capturing data to disk would be: library(IBrokers) tws <-
twsConnect() aapl.csv <- file("AAPL.csv", open="w") # run an
infinite-loop ( <C-c> to break ) reqMktData(tws, twsSTK("AAPL"),
eventWrapper=eWrapper.MktData.CSV(1), file=aapl.csv) ...



http://lmax-exchange.github.com/disruptor/

http://martinfowler.com/articles/lmax.html



**** IB steps

download TWS (or IB gateway)
R IBrokers package

Using TWS 
- Configure > API > Enable ActiveX and Sockets.
- add local machine (127.0.0.1) to the Trusted IP Addresses.

Using IBGateway
- IBGateway application is 4001, which must be changed in the twsConnect call if used.

*** TODO acme rescue
:PROPERTIES:
:ID:       960574BA-3522-45B1-B184-C436FE46C072
:END:

discrete factor model of

growth, inflation, real, cash, bonds, equities

- [ ] remember formal name
- [ ] sort out which acme best
- [ ] clean up
- [ ] git


